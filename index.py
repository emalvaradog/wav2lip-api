# -*- coding: utf-8 -*-
"""wav2lip_api.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GTQtHLhQMWH0qWjH5uGgwVtpnZ1a2Hgy

# FAST API CONFIG
"""

!pip3 install google-cloud-storage
!pip3 install fastapi
!pip3 install pydantic
!pip3 install uvicorn
!pip3 install pyngrok
!pip3 install nest_asyncio
!pip3 install typing
!pip3 install pydub
!pip3 install python-multipart
!pip3 install pypi-json
!pip3 install opencv

"""# wav2lip config"""

# !rm -rf /content/sample_data
# !mkdir /content/sample_data

!git clone https://github.com/zabique/Wav2Lip

#download the pretrained model
!wget 'https://iiitaphyd-my.sharepoint.com/personal/radrabha_m_research_iiit_ac_in/_layouts/15/download.aspx?share=EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA' -O '/content/Wav2Lip/checkpoints/wav2lip_gan.pth'
!wget 'https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/Eb3LEzbfuKlJiR600lQWRxgBIY27JZg80f7V9jtMfbNDaQ?e=TBFBVW' -O '/content/Wav2Lip/checkpoints/wav2lip.pth'
!pip install https://raw.githubusercontent.com/AwaleSajil/ghc/master/ghc-1.0-py3-none-any.whl

# !pip uninstall tensorflow tensorflow-gpu
!cd Wav2Lip && pip install -r requirements.txt

#download pretrained model for face detection
!wget "https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth" -O "/content/Wav2Lip/face_detection/detection/sfd/s3fd.pth"

# API IMPLEMENTATION
import os
from google.cloud import storage

from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.responses import FileResponse
from pydub import AudioSegment
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from os import path
import uvicorn
from pyngrok import ngrok
import nest_asyncio
from typing import List
from typing_extensions import Text
import json
import tempfile

from click.utils import filename_to_ui
from IPython.utils import py3compat
app = FastAPI()

origins = ["*"]

app.add_middleware(CORSMiddleware, allow_origins=origins, allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

preset = "fast"

@app.get("/")
def read_root():
  return {"status": "ok", "message": "Hello World"}


@app.post("/sync")
def lip_sync(audio: UploadFile=File(...), video: UploadFile=File(...)):
  if (audio.filename.find(".wav") == -1):
    return {"status": "failed", "message": "Audio should be .wav"}

  else:
    audio_path = os.path.join("/content/sample_data", audio.filename)
    with open(audio_path, "wb") as f:
      f.write(audio.file.read())
      f.close()

    video_path = os.path.join("/content/sample_data", video.filename)
    with open(video_path, "wb") as f:
      f.write(video.file.read())
      f.close()

    video_path_name = '../sample_data/%s' %(video.filename)
    audio_path_name = '../sample_data/%s' %(audio.filename)
    

    print(video_path_name)
    print(audio_path_name)

    pad_top =  0
    pad_bottom =  25
    pad_left =  -10
    pad_right =  -10
    rescaleFactor =  1

    !cd Wav2Lip 
    !python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face '../sample_data/1.mp4' --audio '../sample_data/2.wav' --pads $pad_top $pad_bottom $pad_left $pad_right
    result_path = "/content/Wav2Lip/results/result_voice.mp4"

    with open(result_path, "rb") as f:
      if f.file:
        size = os.stat(result_path)
        print(size.st_size)
        return FileResponse("/content/Wav2Lip/results/result_voice.mp4")
      else: 
        pass



if __name__ == "__main__":
  PORT = 8000
  ngrok_tunnel = ngrok.connect(PORT)
  print("Public URL: ", ngrok_tunnel.public_url)
  nest_asyncio.apply()
  uvicorn.run(app, host="127.0.0.1", port=PORT)